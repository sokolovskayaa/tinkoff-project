# Учебный проект на курсе "Java Backend-разработка" от Тинькофф Образования. 

### Телеграм бот, который позволяет отслеживать изменения в репозитории/вопросе на GitHub/StackOverflow.

## Описание проекта

### Реализован многомодульный проект, состоящий из 3 частей.
1. [Парсер ссылок](linkParser), отвечающий за разбор приходящих новостей от GitHub и StackOverflow
2. [Scrapper](scrapper), отвечающий за вызов методов API у сервисов GitHub и StackOverflow по поиску новых событий.
3. [Telegram bot](bot), отвечающий за доставку обновлений до пользователя

### LinkParser

В модуле link-parser реализован механизм разбора URL, использующий паттерн "Chain of Responsibility" который:
* Для ссылок с GitHub возвращает пару пользователь/репозиторий
* Для ссылок со StackOverflow возвращает id
* Для остальных ссылок возвращает null

### Scrapper

В модуле scrapper реализована логика отвечающая за обработку запросов от пользователей вида:
* Зарегестрировать пользователя 
* Добавить ссылку, которую он будет отслеживать
* Удалить отслеживаемую ссылку

Также в этом пакете находится [LinkUpdaterScheduler](scrapper/src/main/java/ru/tinkoff/edu/java/scrapper/scheduler/LinkUpdaterScheduler.java), выполняющий запросы в API по определенному таймауту, для получения уведомлений о новых событиях.
Эти уведомления будут отосланы в модуль bot, для дальнейшей отсылке пользователю.

Для хранения чатов и ссылок разработана база данных со связями Many To Many. 
Для работы с бд были написаны репозитории использующие технологии JOOQ, Jpa и Jdbc. Также была настроена автоконфигурация в зависимости от настройки в application.yml.

### Bot

В модуле bot реализован телеграм бот использующий библиотеку pengrad.

Написаны обработчики команд по регистрации, получению всех возможных ссылок, а также по запросам на начало отслеживания ссылки и конец.

Коммуникация между ботом и скраппером реализована в 2 вариантах:
1. Отправка HTTP запросов через клиенты и отлавливание через контроллеры
2. Отправка и считывание с помощью очереди сообщений RabbitMQ.

### База данных
База данных поднимается в докер контейнере. Для поддержания актуального состояния базы данных используется технология накатывания миграций liquibase.

Все скрипты миграции лежат в пакете [migrations](scrapper/migrations).
